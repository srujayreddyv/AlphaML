{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade plotnine mizani","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:53:39.604994Z","iopub.execute_input":"2023-09-25T05:53:39.605421Z","iopub.status.idle":"2023-09-25T05:53:54.397570Z","shell.execute_reply.started":"2023-09-25T05:53:39.605388Z","shell.execute_reply":"2023-09-25T05:53:54.396034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom plotnine import *","metadata":{"id":"7orIedGckTt8","execution":{"iopub.status.busy":"2023-09-25T05:53:57.676979Z","iopub.execute_input":"2023-09-25T05:53:57.677467Z","iopub.status.idle":"2023-09-25T05:53:58.951180Z","shell.execute_reply.started":"2023-09-25T05:53:57.677430Z","shell.execute_reply":"2023-09-25T05:53:58.949907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Understanding","metadata":{"id":"nmmr2_EpuuRL"}},{"cell_type":"code","source":"# Reading the Data\ndf = pd.read_csv(\"/kaggle/input/imdb-prediction-by-123-of-ai-weekend-hackathon/train_data.csv\")","metadata":{"id":"m48BNoQUk9x-","execution":{"iopub.status.busy":"2023-09-25T05:54:01.376885Z","iopub.execute_input":"2023-09-25T05:54:01.377581Z","iopub.status.idle":"2023-09-25T05:54:01.434741Z","shell.execute_reply.started":"2023-09-25T05:54:01.377543Z","shell.execute_reply":"2023-09-25T05:54:01.433492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying the first 10 records\ndf.head(10)","metadata":{"id":"MK2fV7zylGvy","execution":{"iopub.status.busy":"2023-09-25T05:54:02.768667Z","iopub.execute_input":"2023-09-25T05:54:02.769439Z","iopub.status.idle":"2023-09-25T05:54:02.821423Z","shell.execute_reply.started":"2023-09-25T05:54:02.769375Z","shell.execute_reply":"2023-09-25T05:54:02.820171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:04.285359Z","iopub.execute_input":"2023-09-25T05:54:04.286304Z","iopub.status.idle":"2023-09-25T05:54:04.310556Z","shell.execute_reply.started":"2023-09-25T05:54:04.286259Z","shell.execute_reply":"2023-09-25T05:54:04.308442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:05.622268Z","iopub.execute_input":"2023-09-25T05:54:05.623391Z","iopub.status.idle":"2023-09-25T05:54:05.631800Z","shell.execute_reply.started":"2023-09-25T05:54:05.623346Z","shell.execute_reply":"2023-09-25T05:54:05.630442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum(axis = 0)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:06.834334Z","iopub.execute_input":"2023-09-25T05:54:06.834770Z","iopub.status.idle":"2023-09-25T05:54:06.853695Z","shell.execute_reply.started":"2023-09-25T05:54:06.834729Z","shell.execute_reply":"2023-09-25T05:54:06.852122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like our data had 2 null entries for languague field, so i am removing the missing values with dropna() method","metadata":{}},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:08.916494Z","iopub.execute_input":"2023-09-25T05:54:08.917850Z","iopub.status.idle":"2023-09-25T05:54:08.931541Z","shell.execute_reply.started":"2023-09-25T05:54:08.917806Z","shell.execute_reply":"2023-09-25T05:54:08.930248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:10.240076Z","iopub.execute_input":"2023-09-25T05:54:10.240543Z","iopub.status.idle":"2023-09-25T05:54:10.265503Z","shell.execute_reply.started":"2023-09-25T05:54:10.240505Z","shell.execute_reply":"2023-09-25T05:54:10.264056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a histogram of IMDb scores\nfigure1 = df['imdb_score'].hist(bins=100, figsize=[14, 8])\n\n# Set labels and title\nplt.xlabel('imdb_score')\nplt.ylabel('frequency')\nplt.title('Distribution of IMDb Scores')\n\n# Show the histogram\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:11.407997Z","iopub.execute_input":"2023-09-25T05:54:11.409543Z","iopub.status.idle":"2023-09-25T05:54:11.976462Z","shell.execute_reply.started":"2023-09-25T05:54:11.409476Z","shell.execute_reply":"2023-09-25T05:54:11.975506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['imdb_score'].std()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:12.643436Z","iopub.execute_input":"2023-09-25T05:54:12.643906Z","iopub.status.idle":"2023-09-25T05:54:12.653376Z","shell.execute_reply.started":"2023-09-25T05:54:12.643869Z","shell.execute_reply":"2023-09-25T05:54:12.652163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['imdb_score'].var()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:15.301696Z","iopub.execute_input":"2023-09-25T05:54:15.302141Z","iopub.status.idle":"2023-09-25T05:54:15.310918Z","shell.execute_reply.started":"2023-09-25T05:54:15.302087Z","shell.execute_reply":"2023-09-25T05:54:15.309494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between the imdb score and the profit made by the movie\nsns.lmplot(x='imdb_score', y='Profit', data=df, scatter_kws={\"s\": 5})\nplt.xlabel('imdb_score')\nplt.ylabel('profit')\nplt.title('Relationship between IMDb Score and Profit')\nplt.show()","metadata":{"id":"0iwiZtdXlhRf","execution":{"iopub.status.busy":"2023-09-25T05:54:16.488466Z","iopub.execute_input":"2023-09-25T05:54:16.488857Z","iopub.status.idle":"2023-09-25T05:54:17.546315Z","shell.execute_reply.started":"2023-09-25T05:54:16.488826Z","shell.execute_reply":"2023-09-25T05:54:17.544909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Top 20 actors of movies based on the imdb rating of the movies\n\nplt.figure(figsize=(16, 12))\n\n# new dataframe with top 20 values\nnew_df = df.sort_values(by ='imdb_score' , ascending=False)\nnew_df = new_df.head(20)\n\n# plotting\nax=sns.pointplot(new_df, x = 'actor_1_name', y = 'imdb_score', hue= 'movie_title')\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.xlabel('actor_1_name')\nplt.ylabel('imdb_score')\nplt.title('Relationship between top 20 actors of movies based on the imdb rating of the movies')\nplt.tight_layout()\nplt.show()","metadata":{"id":"kNFBi9jyk90_","execution":{"iopub.status.busy":"2023-09-25T05:54:18.093177Z","iopub.execute_input":"2023-09-25T05:54:18.093616Z","iopub.status.idle":"2023-09-25T05:54:20.412415Z","shell.execute_reply.started":"2023-09-25T05:54:18.093583Z","shell.execute_reply":"2023-09-25T05:54:20.411306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.duplicated().sum()    # total sum of duplicate rows","metadata":{"id":"2HzvdnG0mWEZ","execution":{"iopub.status.busy":"2023-09-25T05:54:23.647821Z","iopub.execute_input":"2023-09-25T05:54:23.648293Z","iopub.status.idle":"2023-09-25T05:54:23.673585Z","shell.execute_reply.started":"2023-09-25T05:54:23.648257Z","shell.execute_reply":"2023-09-25T05:54:23.671889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre-processing","metadata":{"id":"x0DNGRl_uyXu"}},{"cell_type":"code","source":"# Correlation with heat map - to find to which feature is similar to which other\n# those above 0.5 corr score have significant overlap in information\n\nnumeric_df = df.select_dtypes(include=['number'])\n\n# calc corr\ncorr = numeric_df.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\nplt.figure(figsize=(16,12))\n\n# create a mask so we only see the correlation values once\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True\na = sns.heatmap(corr, mask=mask, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","metadata":{"id":"syimJXKwl-n2","execution":{"iopub.status.busy":"2023-09-25T05:54:25.688479Z","iopub.execute_input":"2023-09-25T05:54:25.689034Z","iopub.status.idle":"2023-09-25T05:54:27.192844Z","shell.execute_reply.started":"2023-09-25T05:54:25.688988Z","shell.execute_reply":"2023-09-25T05:54:27.191722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def function1(a1, a2):\n    return a1 + a2\n\ndf['other_actors_facebook_likes'] = df.apply(lambda x: function1(x['actor_2_facebook_likes'], x['actor_3_facebook_likes']), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:31.088402Z","iopub.execute_input":"2023-09-25T05:54:31.088807Z","iopub.status.idle":"2023-09-25T05:54:31.176288Z","shell.execute_reply.started":"2023-09-25T05:54:31.088775Z","shell.execute_reply":"2023-09-25T05:54:31.174927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['actor_2_facebook_likes', 'actor_3_facebook_likes' ,'num_voted_users', 'facenumber_in_poster', 'Profit', 'aspect_ratio', 'cast_total_facebook_likes'], axis=1, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:33.418202Z","iopub.execute_input":"2023-09-25T05:54:33.418659Z","iopub.status.idle":"2023-09-25T05:54:33.427728Z","shell.execute_reply.started":"2023-09-25T05:54:33.418622Z","shell.execute_reply":"2023-09-25T05:54:33.426324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:34.856818Z","iopub.execute_input":"2023-09-25T05:54:34.857316Z","iopub.status.idle":"2023-09-25T05:54:34.879704Z","shell.execute_reply.started":"2023-09-25T05:54:34.857278Z","shell.execute_reply":"2023-09-25T05:54:34.878008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Tip 1 - Handle different kinds of data types\n\n### Handling categorical data- country\ndf2 = pd.get_dummies(data = df, columns = ['country'] , prefix = ['country'] , drop_first = True)\n\n### Handling categorical data- content_rating\ndf2 = pd.get_dummies(data = df2, columns = ['content_rating'] , prefix = ['content_rating'] , drop_first = True)\n\n### Handling categorical data- language\ndf2 = pd.get_dummies(data = df2, columns = ['language'] , prefix = ['language'] , drop_first = True)\n\n### Handling categorical data- actor_1_name\ndf2 = pd.get_dummies(data = df2, columns = ['director_name'] , prefix = ['director_name'] , drop_first = True)\n\ndf2.head(10)\n\n\n### Handle different kinds of data - text etc., as you see fit","metadata":{"id":"XdYwC8VWu1j8","execution":{"iopub.status.busy":"2023-09-25T05:54:36.635717Z","iopub.execute_input":"2023-09-25T05:54:36.637078Z","iopub.status.idle":"2023-09-25T05:54:36.778027Z","shell.execute_reply.started":"2023-09-25T05:54:36.636979Z","shell.execute_reply":"2023-09-25T05:54:36.776187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-25T05:54:39.056477Z","iopub.execute_input":"2023-09-25T05:54:39.056871Z","iopub.status.idle":"2023-09-25T05:54:39.169517Z","shell.execute_reply.started":"2023-09-25T05:54:39.056840Z","shell.execute_reply":"2023-09-25T05:54:39.168196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### handling textual data \n\n### Handling categorical data- plot_keywords\ndf3 = df.copy()\ndf3['plot_keywords'] = df3['plot_keywords'].str.split('|')\ndf3['plot_keyword_count'] = df3['plot_keywords'].apply(len)\n\n### Handling categorical data- geners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \ndf3['genres'] = df3['genres'].str.split('|')\ndf3['genres_count'] = df3['genres'].apply(len)\n\ndf3.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:49.068429Z","iopub.execute_input":"2023-09-25T05:54:49.068918Z","iopub.status.idle":"2023-09-25T05:54:49.141255Z","shell.execute_reply.started":"2023-09-25T05:54:49.068878Z","shell.execute_reply":"2023-09-25T05:54:49.139827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:50.827962Z","iopub.execute_input":"2023-09-25T05:54:50.828406Z","iopub.status.idle":"2023-09-25T05:54:50.849029Z","shell.execute_reply.started":"2023-09-25T05:54:50.828372Z","shell.execute_reply":"2023-09-25T05:54:50.847940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation with heat map - to find to which feature is similar to which other\n# those above 0.5 corr score have significant overlap in information\n\nnumeric_df2 = df3.select_dtypes(include=['number'])\n\n# calc corr\ncorr = numeric_df2.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\nplt.figure(figsize=(16,12))\n\n# create a mask so we only see the correlation values once\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True\na = sns.heatmap(corr, mask=mask, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:52.226204Z","iopub.execute_input":"2023-09-25T05:54:52.227740Z","iopub.status.idle":"2023-09-25T05:54:53.327809Z","shell.execute_reply.started":"2023-09-25T05:54:52.227683Z","shell.execute_reply":"2023-09-25T05:54:53.326516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Tip 2 - Based on above data analysis, choose all or relevant features\n### Tip 3 - Feel free to extract most important features using PCA, regularisation, above correlation heatmaps etc.\n### Feel free to convert the text to textual feature vectors, and use those as input too.","metadata":{"id":"RNpsZlkxu6iI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_df2.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:55.870473Z","iopub.execute_input":"2023-09-25T05:54:55.870909Z","iopub.status.idle":"2023-09-25T05:54:55.880596Z","shell.execute_reply.started":"2023-09-25T05:54:55.870875Z","shell.execute_reply":"2023-09-25T05:54:55.878966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_df2.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:54:57.323354Z","iopub.execute_input":"2023-09-25T05:54:57.323772Z","iopub.status.idle":"2023-09-25T05:54:57.339783Z","shell.execute_reply.started":"2023-09-25T05:54:57.323740Z","shell.execute_reply":"2023-09-25T05:54:57.338345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tip 5- Don't forget feature scaling; check K-NN bootcamp text\n# standardization (Z-score scaling)\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Scale the 'duration' column\nnumeric_df2['duration'] = scaler.fit_transform(numeric_df2[['duration']])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:55:07.062953Z","iopub.execute_input":"2023-09-25T05:55:07.064290Z","iopub.status.idle":"2023-09-25T05:55:07.076705Z","shell.execute_reply.started":"2023-09-25T05:55:07.064240Z","shell.execute_reply":"2023-09-25T05:55:07.075069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_df2.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:55:08.476135Z","iopub.execute_input":"2023-09-25T05:55:08.476943Z","iopub.status.idle":"2023-09-25T05:55:08.493274Z","shell.execute_reply.started":"2023-09-25T05:55:08.476892Z","shell.execute_reply":"2023-09-25T05:55:08.491740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_df2.duplicated().sum()    # total sum of duplicate rows","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:55:10.159503Z","iopub.execute_input":"2023-09-25T05:55:10.160222Z","iopub.status.idle":"2023-09-25T05:55:10.175020Z","shell.execute_reply.started":"2023-09-25T05:55:10.160174Z","shell.execute_reply":"2023-09-25T05:55:10.173702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## regularisation model for feature selcection ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Tip 4 - pandas data frames can be directly used in train and test split creation\nx = numeric_df2.drop(['imdb_score'], axis=1)  # Features\ny = numeric_df2['imdb_score']  # Target variable\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=100)","metadata":{"id":"0cFDV9kAoHbx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nw = []\nr2 = []\nalpha_list = [0.001, 0.01, 0.1, 1.0, 10.0]\n\nfor alpha in alpha_list:\n  #fit model\n  model = Lasso(alpha=alpha)\n  model.fit(X_train, y_train)\n\n  #prediction\n  y_pred = model.predict(X_test)\n\n  #store metrics for prediction for each fit\n  r2.append(r2_score(y_test, y_pred))\n  w.append(model.coef_.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pd.DataFrame({'w': w}))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for coef in w:\n    # Initialize a Linear Regression model with the Lasso coefficients\n    model = LinearRegression()\n    model.coef_ = coef  # Set the coefficients\n\n    # Fit the model to the training data\n    model.fit(X_train, y_train)\n\n    # Make predictions on the test data\n    y_pred = model.predict(X_test)\n\n    # Evaluate the model's performance\n    mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n    r2 = r2_score(y_test, y_pred)            # R-squared\n\n    # Print the model's performance metrics\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nprint(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Selection","metadata":{"id":"CK19wkDavVMq"}},{"cell_type":"code","source":"### Experiment with different regression models\n### https://scikit-learn.org/stable/supervised_learning.html","metadata":{"id":"-cAR6QXcvWSC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nx = numeric_df2.drop(['imdb_score'], axis=1)  # Features\ny = numeric_df2['imdb_score']  # Target variable\n\n# Perform PCA to reduce dimensionality to 4 components\npca = PCA(n_components=4)\npca_result = pca.fit_transform(x)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(pca_result, y, test_size=0.2, random_state=5)\n\n# Train a Linear Regression model\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\n# Make predictions using Linear Regression\ny_pred_lm = lm.predict(X_test)\n\n# Calculate RMSE for Linear Regression\nrmse_lm = np.sqrt(mean_squared_error(y_test, y_pred_lm))\nprint(\"RMSE for Linear Regression:\", rmse_lm)\n\n# Calculate R-squared for Linear Regression\nr2_lm = r2_score(y_test, y_pred_lm)\nprint(\"R-squared for Linear Regression:\", r2_lm * 100)\n\n# Train a Random Forest Regressor model\nrf = RandomForestRegressor(random_state=5, max_depth=1000)\nrf.fit(X_train, y_train)\n\n# Make predictions using Random Forest Regressor\ny_pred_rf = rf.predict(X_test)\n\n# Calculate RMSE for Random Forest Regressor\nrmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"RMSE for Random Forest Regressor:\", rmse_rf)\n\n# Calculate R-squared for Random Forest Regressor\nr2_rf = r2_score(y_test, y_pred_rf)\nprint(\"R-squared for Random Forest Regressor:\", r2_rf * 100)\n\n# Train an XGBoost Regressor model with cross-validation\nxgb_reg = xgb.XGBRegressor(random_state=5, max_depth=3, n_estimators=100)\nscores = cross_val_score(xgb_reg, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n\n# Calculate RMSE for XGBoost with cross-validation\nrmse_xgb = np.sqrt(-scores)\nprint(\"RMSE for XGBoost with Cross-Validation:\", rmse_xgb)\n\n# Fit the XGBoost model on the full training data\nxgb_reg.fit(X_train, y_train)\n\n# Make predictions using XGBoost\ny_pred_xgb = xgb_reg.predict(X_test)\n\n# Calculate R-squared for XGBoost\nr2_xgb = r2_score(y_test, y_pred_xgb)\nprint(\"R-squared for XGBoost:\", r2_xgb * 100)","metadata":{"id":"QOnOqPbcvctJ","execution":{"iopub.status.busy":"2023-09-25T05:55:14.563152Z","iopub.execute_input":"2023-09-25T05:55:14.564462Z","iopub.status.idle":"2023-09-25T05:55:17.289569Z","shell.execute_reply.started":"2023-09-25T05:55:14.564422Z","shell.execute_reply":"2023-09-25T05:55:17.288643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation and saving output","metadata":{"id":"7Zb-W-KzvyzX"}},{"cell_type":"code","source":"# Creating output file for submission - Template Code\n\ntest = pd.read_csv('/kaggle/input/imdb-prediction-by-123-of-ai-weekend-hackathon/test_data_with_inputs.csv')\n\ndef fun1(a1, a2):\n    return a1 + a2\n\ntest['other_actors_facebook_likes'] = test.apply(lambda x: fun1(x['actor_2_facebook_likes'], x['actor_3_facebook_likes']), axis=1)\n\ntest.drop(['actor_2_facebook_likes', 'actor_3_facebook_likes' ,'num_voted_users', 'facenumber_in_poster', 'Profit', 'aspect_ratio', 'cast_total_facebook_likes'], axis=1, inplace=True)\n\n### Handling categorical data- plot_keywords\ntest['plot_keywords'] = test['plot_keywords'].str.split('|')\ntest['plot_keyword_count'] = test['plot_keywords'].apply(len)\n\n### Handling categorical data- geners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \ntest['genres'] = test['genres'].str.split('|')\ntest['genres_count'] = test['genres'].apply(len)\n\nnumeric_test = test.select_dtypes(include=['number'])\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Scale the 'duration' column\nnumeric_test['duration'] = scaler.fit_transform(numeric_test[['duration']])\n\nnumeric_test.info()\n\n# Perform PCA to reduce dimensionality to 4 components\npca = PCA(n_components=4)\npca_result_test = pca.fit_transform(numeric_test)\n\n# Convert all submission data to same input format as done for train data\n# run prediction as y_pred = model.predict(X_test)\ny_pred_xgb = xgb_reg.predict(pca_result_test)\n# y_pred contains IMDB scores\n\nsubmission = pd.DataFrame({'s_no':test.s_no, 'imdb_score':y_pred_rf[..., 0]}).set_index('s_no')\nsubmission.to_csv('output_submission.csv')","metadata":{"id":"X833FyDYv0IY","execution":{"iopub.status.busy":"2023-09-25T05:55:22.874747Z","iopub.execute_input":"2023-09-25T05:55:22.875298Z","iopub.status.idle":"2023-09-25T05:55:22.955567Z","shell.execute_reply.started":"2023-09-25T05:55:22.875262Z","shell.execute_reply":"2023-09-25T05:55:22.954473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"xRKZLwnaoFQf"},"execution_count":null,"outputs":[]}]}